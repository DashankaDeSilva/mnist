{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preproc(x):\n",
    "    x = x*2 - 1.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    def __init__(self, sess, model):\n",
    "        self.model = model\n",
    "        self.sess = sess\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        feed = {\n",
    "            self.model.X: X,\n",
    "            self.model.y: y,\n",
    "            self.model.training: True\n",
    "        }\n",
    "        train_op = self.model.train_op\n",
    "        loss = self.model.loss\n",
    "        \n",
    "        return self.sess.run([train_op, loss], feed_dict=feed)\n",
    "    \n",
    "    def evaluate(self, X, y, batch_size=None):\n",
    "        if batch_size:\n",
    "            N = X.shape[0]\n",
    "            \n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "            \n",
    "            for i in range(0, N, batch_size):\n",
    "                X_batch = X[i:i + batch_size]\n",
    "                y_batch = y[i:i + batch_size]\n",
    "                \n",
    "                feed = {\n",
    "                    self.model.X: X_batch,\n",
    "                    self.model.y: y_batch,\n",
    "                    self.model.training: False\n",
    "                }\n",
    "                \n",
    "                loss = self.model.loss\n",
    "                accuracy = self.model.accuracy\n",
    "                \n",
    "                step_loss, step_acc = self.sess.run([loss, accuracy], feed_dict=feed)\n",
    "                \n",
    "                total_loss += step_loss * X_batch.shape[0]\n",
    "                total_acc += step_acc * X_batch.shape[0]\n",
    "            \n",
    "            total_loss /= N\n",
    "            total_acc /= N\n",
    "            \n",
    "            return total_loss, total_acc\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            feed = {\n",
    "                self.model.X: X,\n",
    "                self.model.y: y,\n",
    "                self.model.training: False\n",
    "            }\n",
    "            \n",
    "            loss = self.model.loss            \n",
    "            accuracy = self.model.accuracy\n",
    "\n",
    "            return self.sess.run([loss, accuracy], feed_dict=feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, name, lr=0.001):\n",
    "        with tf.variable_scope(name):\n",
    "            self.X = tf.placeholder(tf.float32, [None, 784], name='X')\n",
    "            self.y = tf.placeholder(tf.float32, [None, 10], name='y')\n",
    "            self.training = tf.placeholder(tf.bool, name='training')\n",
    "            \n",
    "            x = preproc(self.X)\n",
    "            x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
    "            \n",
    "#             h1_conv = tf.layers.conv2d(x_img, 64, [5,5], strides=2, padding='SAME', use_bias=False)\n",
    "#             h1_bn = tf.layers.batch_normalization(h1_conv, training=self.training)\n",
    "#             h1 = tf.nn.relu(h1_bn) # 14x14\n",
    "#             h2_conv = tf.layers.conv2d(h1, 128, [5,5], strides=2, padding='SAME', use_bias=False)\n",
    "#             h2_bn = tf.layers.batch_normalization(h2_conv, training=self.training)\n",
    "#             h2 = tf.nn.relu(h2_bn) # 7x7\n",
    "#             h3_conv = tf.layers.conv2d(h2, 256, [5,5], strides=2, padding='SAME', use_bias=False)\n",
    "#             h3_bn = tf.layers.batch_normalization(h3_conv, training=self.training)\n",
    "#             h3 = tf.nn.relu(h3_bn) # 4x4\n",
    "            \n",
    "            # hidden layers\n",
    "            net = x_img\n",
    "            n_filters = 64\n",
    "            for i in range(3):\n",
    "                net = tf.layers.conv2d(net, n_filters, [3,3], strides=1, kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                       padding='SAME', use_bias=False)\n",
    "                net = tf.layers.batch_normalization(net, training=self.training)\n",
    "                net = tf.nn.relu(net)\n",
    "                net = tf.layers.dropout(net, rate=0.3, training=self.training)\n",
    "                \n",
    "                net = tf.layers.conv2d(net, n_filters, [5,5], strides=2, kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                       padding='SAME', use_bias=False)\n",
    "#                 net = tf.layers.max_pooling2d(net, pool_size=[2,2], strides=2)\n",
    "                net = tf.layers.batch_normalization(net, training=self.training)\n",
    "                net = tf.nn.relu(net)\n",
    "                net = tf.layers.dropout(net, rate=0.3, training=self.training)\n",
    "                n_filters *= 2\n",
    "            \n",
    "            # x: [28, 28, 1]\n",
    "            # h1: [14, 14, 64]\n",
    "            # h2: [7, 7, 128]\n",
    "            # h3: [4, 4, 256]\n",
    "            # 4096 -> 1024 -> 10\n",
    "            \n",
    "            net = tf.contrib.layers.flatten(net)\n",
    "#             net = tf.layers.dense(net, 1024, activation=tf.nn.relu)\n",
    "#             net = tf.layers.dropout(net, rate=0.5, training=self.training)\n",
    "            logits = tf.layers.dense(net, 10, weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.y))\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=name)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                self.train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(self.loss)    \n",
    "            \n",
    "            self.pred = tf.argmax(logits, axis=1)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self.pred, tf.argmax(self.y, axis=1)), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "basic_cnn = Model('basic_cnn', lr=0.001)\n",
    "solver = Solver(sess, basic_cnn)\n",
    "\n",
    "tf.set_random_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/60] train: 0.0675, 98.113% / valid: 0.0682, 98.18% / test: 0.0521, 98.47%\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batch_size = 50\n",
    "epoch_n = 60\n",
    "N = mnist.train.num_examples\n",
    "\n",
    "max_train_acc = 0\n",
    "max_valid_acc = 0\n",
    "max_test_acc = 0\n",
    "\n",
    "for epoch in range(epoch_n):\n",
    "    for _ in range(N // batch_size):\n",
    "        batches = mnist.train.next_batch(batch_size)\n",
    "        _, train_loss = solver.train(batches[0], batches[1])\n",
    "#         sess.run(solver, {X: batches[0], y: batches[1]})\n",
    "    \n",
    "    train_loss, train_acc = solver.evaluate(mnist.train.images, mnist.train.labels, 1000)\n",
    "    valid_loss, valid_acc = solver.evaluate(mnist.validation.images, mnist.validation.labels, 1000)\n",
    "    test_loss, test_acc = solver.evaluate(mnist.test.images, mnist.test.labels, 1000)\n",
    "    line = \"[{:0>2d}/{}] train: {:.4f}, {:.3%} / valid: {:.4f}, {:.2%} / test: {:.4f}, {:.2%}\". \\\n",
    "    format(epoch+1, epoch_n, train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc)\n",
    "    print(line)\n",
    "    \n",
    "    if train_acc > max_train_acc:\n",
    "        max_train_acc = train_acc\n",
    "        train_line = line\n",
    "    if valid_acc > max_valid_acc:\n",
    "        max_valid_acc = valid_acc\n",
    "        valid_line = line\n",
    "    if test_acc > max_test_acc:\n",
    "        max_test_acc = test_acc\n",
    "        test_line = line\n",
    "    \n",
    "\n",
    "print(\"[train max] {}\".format(train_line))\n",
    "print(\"[valid max] {}\".format(valid_line))\n",
    "print(\"[ test max] {}\".format(test_line))\n",
    "# print(\"last maximum train acc: {:.2%}\".format(max_train_acc))\n",
    "# print(\"last maximum valid acc: {:.2%}\".format(max_valid_acc))\n",
    "# print(\"last maximum test acc: {:.2%}\".format(max_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train max] [59/60] train: 0.0001, 100.000% / valid: 0.0298, 99.54% / test: 0.0228, 99.51%\n",
      "[valid max] [43/60] train: 0.0013, 99.949% / valid: 0.0289, 99.56% / test: 0.0333, 99.33%\n",
      "[ test max] [48/60] train: 0.0004, 99.984% / valid: 0.0362, 99.42% / test: 0.0246, 99.61%\n"
     ]
    }
   ],
   "source": [
    "print(\"[train max] {}\".format(train_line))\n",
    "print(\"[valid max] {}\".format(valid_line))\n",
    "print(\"[ test max] {}\".format(test_line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "* basic-without preproc(zero-centered mean): 98.60%\n",
    "* basic: 98.95%\n",
    "* BN: 98.68%\n",
    "    * BN-0.01: 94%\n",
    "    * BN-0.05: 98.29%\n",
    "\n",
    "\n",
    "* 2-strided models\n",
    "    * BN: 99.26%\n",
    "        * bias: 99.15%\n",
    "    * No-BN, No-bias: 99.10%\n",
    "    * No-BN: 99.26%\n",
    "        * added 1 more 1024 dense layer: 99.00%\n",
    "        * normalized input: 99.15%\n",
    "* [(3,3),1] + [(5,5),2] model + BN\n",
    "    * 2-FC + dropout: 99.39%, 99.46%\n",
    "    * 1-FC: 99.52%, 99.49%\n",
    "        * conv dropout 0.5: 99.53%\n",
    "        * conv dropout 0.2: 99.45?\n",
    "        * conv dropout 0.7: 99.11%, 99.35% (batch size 50)\n",
    "        * conv dropout 0.3 + batch size 50 + epoch 30: 99.55%\n",
    "* [(3,3),1] + [(3,3),2] model + BN\n",
    "    * 1-FC\n",
    "        * conv dropout 0.1: 99.41%\n",
    "* max pooling 99.35%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
